{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db302962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import string\n",
    "import mlflow \n",
    "import mlflow.sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbbf55bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2a9a9804-fd34-41cd-9ce0-5ea6aed3baea",
       "rows": [
        [
         "0",
         "empty",
         "@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =["
        ],
        [
         "1",
         "sadness",
         "Layin n bed with a headache  ughhhh...waitin on your call..."
        ],
        [
         "2",
         "sadness",
         "Funeral ceremony...gloomy friday..."
        ],
        [
         "3",
         "enthusiasm",
         "wants to hang out with friends SOON!"
        ],
        [
         "4",
         "neutral",
         "@dannycastillo We want to trade with someone who has Houston tickets, but no one will."
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv').drop(columns=['tweet_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fe2f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/g_/sy0vrt390g59cw_vcq84l2br0000gn/T/ipykernel_32131/936982704.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  text = re.sub('\\s+', ' ', text).strip()\n"
     ]
    }
   ],
   "source": [
    "#data_preprocessing\n",
    "\n",
    "def lemmatization(text):\n",
    "    \"\"\"Lemmatize the text.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"Remove stop words from the text.\"\"\"\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text = [word for word in str(text).split() if word not in stop_words]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_numbers(text):\n",
    "    \"\"\"Remove numbers from the text.\"\"\"\n",
    "    text = ''.join([char for char in text if not char.isdigit()])\n",
    "    return text\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"Convert text to lower case.\"\"\"\n",
    "    text = text.split()\n",
    "    text = [word.lower() for word in text]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def removing_punctuations(text):\n",
    "    \"\"\"Remove punctuations from the text.\"\"\"\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = text.replace('؛', \"\")\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def removing_urls(text):\n",
    "    \"\"\"Remove URLs from the text.\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "def normalize_text(df:pd.DataFrame):\n",
    "    \"\"\"Normalize the text data.\"\"\"\n",
    "    try:\n",
    "        df['content'] = df['content'].apply(lower_case)\n",
    "        df['content'] = df['content'].apply(remove_stop_words)\n",
    "        df['content'] = df['content'].apply(removing_numbers)\n",
    "        df['content'] = df['content'].apply(removing_punctuations)\n",
    "        df['content'] = df['content'].apply(removing_urls)\n",
    "        df['content'] = df['content'].apply(lemmatization)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f'Error during text normalization: {e}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053517bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=normalize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "450e3e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "dcbe54b7-224c-40f8-b72b-c299242b4c8c",
       "rows": [
        [
         "0",
         "empty",
         "tiffanylue know listenin bad habit earlier started freakin part"
        ],
        [
         "1",
         "sadness",
         "layin n bed headache ughhhh waitin call"
        ],
        [
         "2",
         "sadness",
         "funeral ceremony gloomy friday"
        ],
        [
         "3",
         "enthusiasm",
         "want hang friend soon"
        ],
        [
         "4",
         "neutral",
         "dannycastillo want trade someone houston ticket one will"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>tiffanylue know listenin bad habit earlier sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>want hang friend soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dannycastillo want trade someone houston ticke...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  tiffanylue know listenin bad habit earlier sta...\n",
       "1     sadness            layin n bed headache ughhhh waitin call\n",
       "2     sadness                     funeral ceremony gloomy friday\n",
       "3  enthusiasm                              want hang friend soon\n",
       "4     neutral  dannycastillo want trade someone houston ticke..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747fc535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "sentiment",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "f6959442-722a-4d8c-afab-37e00926b352",
       "rows": [
        [
         "neutral",
         "8638"
        ],
        [
         "worry",
         "8459"
        ],
        [
         "happiness",
         "5209"
        ],
        [
         "sadness",
         "5165"
        ],
        [
         "love",
         "3842"
        ],
        [
         "surprise",
         "2187"
        ],
        [
         "fun",
         "1776"
        ],
        [
         "relief",
         "1526"
        ],
        [
         "hate",
         "1323"
        ],
        [
         "empty",
         "827"
        ],
        [
         "enthusiasm",
         "759"
        ],
        [
         "boredom",
         "179"
        ],
        [
         "anger",
         "110"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 13
       }
      },
      "text/plain": [
       "sentiment\n",
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3f1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['sentiment'].isin(['happiness','sadness'])\n",
    "df = df[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc5e315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g_/sy0vrt390g59cw_vcq84l2br0000gn/T/ipykernel_32131/468518138.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n",
      "/var/folders/g_/sy0vrt390g59cw_vcq84l2br0000gn/T/ipykernel_32131/468518138.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].replace({'sadness':0, 'happiness':1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00930fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentiment",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "faf1aa13-bcc8-40c3-a564-578607709a0c",
       "rows": [
        [
         "1",
         "0",
         "layin n bed headache ughhhh waitin call"
        ],
        [
         "2",
         "0",
         "funeral ceremony gloomy friday"
        ],
        [
         "6",
         "0",
         "sleep im not thinking old friend want married now damn amp want scandalous"
        ],
        [
         "8",
         "0",
         "charviray charlene love miss"
        ],
        [
         "9",
         "0",
         "kelcouch sorry least friday"
        ],
        [
         "12",
         "0",
         "ugh beat stupid song get next rude"
        ],
        [
         "13",
         "0",
         "brodyjenner u watch hill london u realise tourture week week late watch itonlinelol"
        ],
        [
         "15",
         "0",
         "storm electricity gone"
        ],
        [
         "17",
         "0",
         "sleepy even late fail again"
        ],
        [
         "19",
         "0",
         "convinced always wanted you signal give off damn think lost another friend"
        ],
        [
         "24",
         "0",
         "tired think definitely going get ear infection going bed quot early quot once"
        ],
        [
         "26",
         "0",
         "isaacmascote sorry people rude you isaac get manner know better lewd"
        ],
        [
         "28",
         "0",
         "fudge b d whole paper tired ugh hate school time sleep"
        ],
        [
         "36",
         "0",
         "ether radio yeah s feel funny cause slept enough woke mum cause singing impressed s you"
        ],
        [
         "40",
         "1",
         "mmm much better day far still quite early last day uds"
        ],
        [
         "47",
         "0",
         "problem photo twitter amf can t see face"
        ],
        [
         "59",
         "0",
         "maternitytees aww onward upwards now yay still sad leave bet"
        ],
        [
         "61",
         "0",
         "diesel yaris mpg sad available u that d awesome"
        ],
        [
         "62",
         "0",
         "want buy great album unfortunately dont hav enuff fund quot long time noisy quot"
        ],
        [
         "63",
         "0",
         "pokinatcha honesty pain blech"
        ],
        [
         "64",
         "0",
         "ok passenger one alive dead know til end cry"
        ],
        [
         "67",
         "0",
         "vincew stefanyngo fell asleep beach put enough sunscreen lol"
        ],
        [
         "69",
         "1",
         "great see oin amp cynthia happy dinner great cute little place bad oin got sick afterwards"
        ],
        [
         "77",
         "1",
         "havingmysay dude favorite sandwich place ever ummm take picture"
        ],
        [
         "80",
         "0",
         "rachellock ohh thursday exam day wednesday"
        ],
        [
         "82",
         "0",
         "gcrush nopantsdance thinking excited guy move realized sad see go"
        ],
        [
         "84",
         "0",
         "artfuldodga love it sakey usb stick gb australia"
        ],
        [
         "95",
         "0",
         "sweeetnspicy hiii im ipod i cant fall asleep"
        ],
        [
         "96",
         "0",
         "dont wanna work tomorrow get paid"
        ],
        [
         "97",
         "0",
         "feel sad coz wasnt able play guy http plurk com p wxiux"
        ],
        [
         "103",
         "0",
         "darn allergy like time year this never used problem either"
        ],
        [
         "106",
         "0",
         "cayogial wanted come bz summer sure anymore teacher s life summer suck"
        ],
        [
         "109",
         "0",
         "wish knew put through she stole heart never gave back and occasionally like like look have"
        ],
        [
         "116",
         "0",
         "neesabear early happy day birth case make it tired therapy today n taking medicine misshu love ya"
        ],
        [
         "119",
         "0",
         "allergy suck duck nut lt gt"
        ],
        [
         "125",
         "0",
         "wonder karma point turned http plurk com p wxj"
        ],
        [
         "126",
         "1",
         "need pack cali cali cannot waittt thinking glass wine order celebrate weekend vaca still work morrow tho"
        ],
        [
         "127",
         "0",
         "miserable feel like im gona cry sux"
        ],
        [
         "132",
         "0",
         "sigh going bed feel right anymore"
        ],
        [
         "135",
         "0",
         "depressed watching think dance"
        ],
        [
         "136",
         "0",
         "cross cause stuck twiddling thumb now ugh"
        ],
        [
         "150",
         "0",
         "last day working uni today sad time"
        ],
        [
         "162",
         "0",
         "still missing husband really want home"
        ],
        [
         "163",
         "0",
         "miss puppy"
        ],
        [
         "166",
         "0",
         "nzdeany given pizza kid would never let fav the hot one"
        ],
        [
         "168",
         "0",
         "barbschaefer yearling pet home died sad whole family"
        ],
        [
         "171",
         "0",
         "heresmyhello hate change course fan life lt oh met rob something went horribly wrong"
        ],
        [
         "173",
         "0",
         "well fuck new pain med odd warning actually applies me can t take this chan displeased"
        ],
        [
         "174",
         "0",
         "uploaded new blog painful story s year old man cried wanted die sad"
        ],
        [
         "183",
         "0",
         "saw pic past thanksgiving sad grandma them"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 10374
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>layin n bed headache ughhhh waitin call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>funeral ceremony gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>sleep im not thinking old friend want married ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>charviray charlene love miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>kelcouch sorry least friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39986</th>\n",
       "      <td>1</td>\n",
       "      <td>going watch boy striped pj s hope cry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39987</th>\n",
       "      <td>1</td>\n",
       "      <td>gave bike thorough wash degrease grease it thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>1</td>\n",
       "      <td>amazing time last night mcfly incredible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>1</td>\n",
       "      <td>succesfully following tayla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1</td>\n",
       "      <td>niariley wassup beautiful follow me peep new h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10374 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            content\n",
       "1              0            layin n bed headache ughhhh waitin call\n",
       "2              0                     funeral ceremony gloomy friday\n",
       "6              0  sleep im not thinking old friend want married ...\n",
       "8              0                       charviray charlene love miss\n",
       "9              0                        kelcouch sorry least friday\n",
       "...          ...                                                ...\n",
       "39986          1              going watch boy striped pj s hope cry\n",
       "39987          1  gave bike thorough wash degrease grease it thi...\n",
       "39988          1           amazing time last night mcfly incredible\n",
       "39994          1                        succesfully following tayla\n",
       "39998          1  niariley wassup beautiful follow me peep new h...\n",
       "\n",
       "[10374 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08575a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2dcde3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a1793a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as yadavdipu296\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as yadavdipu296\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"yadavdipu296/mlops\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"yadavdipu296/mlops\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository yadavdipu296/mlops initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository yadavdipu296/mlops initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import dagshub\n",
    "\n",
    "dagshub.init(repo_owner='yadavdipu296', repo_name='mlops', mlflow=True)\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/yadavdipu296/mlops.mlflow\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b65b2348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/6425cc17ab1c453b896a2f8d1a320e76', creation_time=1754294889027, experiment_id='1', last_update_time=1754294889027, lifecycle_stage='active', name='logistic_regression-experiment', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment('logistic_regression-experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944913f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook exp1_baseline_model.ipynb to notebook\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/bin/jupyter-nbconvert\", line 11, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/jupyter_core/application.py\", line 283, in launch_instance\n",
      "    super().launch_instance(argv=argv, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 420, in start\n",
      "    self.convert_notebooks()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 597, in convert_notebooks\n",
      "    self.convert_single_notebook(notebook_filename)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 563, in convert_single_notebook\n",
      "    output, resources = self.export_single_notebook(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/nbconvertapp.py\", line 487, in export_single_notebook\n",
      "    output, resources = self.exporter.from_filename(\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 201, in from_filename\n",
      "    return self.from_file(f, resources=resources, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 220, in from_file\n",
      "    return self.from_notebook_node(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/notebook.py\", line 36, in from_notebook_node\n",
      "    nb_copy, resources = super().from_notebook_node(nb, resources, **kw)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 154, in from_notebook_node\n",
      "    nb_copy, resources = self._preprocess(nb_copy, resources)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/exporters/exporter.py\", line 353, in _preprocess\n",
      "    nbc, resc = preprocessor(nbc, resc)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/base.py\", line 48, in __call__\n",
      "    return self.preprocess(nb, resources)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py\", line 103, in preprocess\n",
      "    self.preprocess_cell(cell, resources, index)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbconvert/preprocessors/execute.py\", line 124, in preprocess_cell\n",
      "    cell = self.execute_cell(cell, index, store_history=True)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/jupyter_core/utils/__init__.py\", line 165, in wrapped\n",
      "    return loop.run_until_complete(inner)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\n",
      "    return future.result()\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbclient/client.py\", line 1058, in async_execute_cell\n",
      "    await self._check_raise_for_error(cell, cell_index, exec_reply)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/nbclient/client.py\", line 914, in _check_raise_for_error\n",
      "    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)\n",
      "nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:\n",
      "------------------\n",
      "import pandas as pd\n",
      "from nltk.corpus import stopwords\n",
      "from  nltk.stem import WordNetLemmatizer\n",
      "import os\n",
      "import regex as re\n",
      "import numpy as np\n",
      "import string\n",
      "import mlflow \n",
      "import mlflow.sklearn\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
      "\n",
      "\n",
      "------------------\n",
      "\n",
      "\n",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n",
      "\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m \n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlflow'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7773493975903615\n",
      "Precision: 0.7692307692307693\n",
      "Recall: 0.7783251231527094\n",
      "F1 Score: 0.7737512242899118\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='logistic_regression'):\n",
    "    mlflow.log_param('Vectorizer', 'Bag of Words')\n",
    "    mlflow.log_param('Max Features', 1000)\n",
    "    mlflow.log_param('test_size', 0.2)\n",
    "    model=LogisticRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    mlflow.log_param('Model', 'Logistic Regression')\n",
    "\n",
    "     # Model evaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric('Accuracy', accuracy)\n",
    "    mlflow.log_metric('Precision', precision)\n",
    "    mlflow.log_metric('Recall', recall)\n",
    "    mlflow.log_metric('F1 Score', f1)\n",
    "\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "    import os\n",
    "\n",
    "    file_path='exp1_baseline_model.ipynb'\n",
    "    os.system(f\"jupyter nbconvert --to notebook --execute --inplace {file_path}\")\n",
    "    mlflow.log_artifact(file_path)\n",
    "\n",
    "\n",
    "    # Print the results for verification\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13beb0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
